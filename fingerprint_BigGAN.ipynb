{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "fingerprint_BigGAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qbEW1V9etTtY"
      },
      "source": [
        "# Fingerprint BigGAN\n",
        "\n",
        "View source on [GitHub](https://github.com/Luksalos/BIO-fingerprint-GAN/). Based on [this Kaggle kernel](https://www.kaggle.com/yukia18/sub-rals-ac-biggan-with-minibatchstddev). [Model description](https://www.kaggle.com/c/generative-dog-images/discussion/104211#latest-601531)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "USu-E8Abt8sc"
      },
      "source": [
        "## Getting the dataset\n",
        "\n",
        "Download the SOCOFing dataset (`socofing.zip`) from [Kaggle](https://www.kaggle.com/ruizgara/socofing)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "colab_type": "text",
        "collapsed": true,
        "id": "V85Omft53the"
      },
      "source": [
        "## Configuration\n",
        "\n",
        "Here you can tweak parameters of the model and specify where you want to run computations and store dataset with results.\n",
        "\n",
        "You can choose between Google Colab with Google Drive, or your local machine:\n",
        "\n",
        "### Using Google Colab and Google Drive\n",
        "\n",
        "If the config below contains `'use_google_drive': True`, you will be asked to provide access to your Google Drive.\n",
        "\n",
        "Your Google Drive should contain the `socofing.zip` file at the top level. This notebook extracts the archive to disk of the Colab runtime, meaning your Google Drive will not be affected.\n",
        "\n",
        "Checkpoints of the GAN and generated samples will be stored in your Drive in the `BigGAN_outputs/` directory.\n",
        "\n",
        "### Using your local machine\n",
        "\n",
        "If the config below contains `'use_google_drive': False`, the `socofing.zip` file should be located in your home directory.\n",
        "\n",
        "The archive will be extracted to `~/socofing/`, checkpoints of the GAN and generated samples will be stored in the `~/BigGAN_outputs/` directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mIvmL1b23thf",
        "colab": {}
      },
      "source": [
        "config = {\n",
        "    'DataLoader': {\n",
        "        'batch_size': 128,\n",
        "        'shuffle': True,\n",
        "    },\n",
        "    'Generator': {\n",
        "        'latent_dim': 120,\n",
        "        'embed_dim': 8,\n",
        "        'ch': 64,\n",
        "        'num_classes': 20,\n",
        "        'use_attn': True,\n",
        "    },\n",
        "    'Discriminator': {\n",
        "        'ch': 64,\n",
        "        'num_classes': 20,\n",
        "        'use_attn': True,\n",
        "    },\n",
        "    'sample_latents': {\n",
        "        'latent_dim': 120,\n",
        "        'num_classes': 20,\n",
        "    },\n",
        "    'num_iterations': 25000,\n",
        "    'decay_start_iteration': 20000,\n",
        "    'd_steps': 1,\n",
        "    'lr_G': 1e-4,\n",
        "    'lr_D': 2e-4,\n",
        "    'betas': (0.0, 0.999),\n",
        "    'margin': 1.0,\n",
        "    'gamma': 0.1,\n",
        "    'ema': 0.999,\n",
        "    'seed': 42,\n",
        "    # 'input_transformation': 'resize-64',          # resize - 64x64\n",
        "    #  'input_transformation': 'crop-64',            # crop - 64x64\n",
        "    #  'input_transformation': 'random-crop-64',     # crop - 64x64\n",
        "     'input_transformation': 'crop-80',            # crop - 80x80\n",
        "    # 'input_transformation': 'crop-96',            # crop - 96x96\n",
        "    'use_google_drive': True,\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1f07ZhkjORg",
        "colab_type": "text"
      },
      "source": [
        "## Libraries and directories setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "colab_type": "code",
        "id": "13dGjbBW3tha",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import random\n",
        "import glob\n",
        "import shutil\n",
        "import warnings\n",
        "import math\n",
        "import pprint\n",
        "import zipfile\n",
        "import pathlib\n",
        "\n",
        "import cv2\n",
        "import scipy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as img\n",
        "import xml.etree.ElementTree as ET\n",
        "import albumentations as A\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from PIL import Image\n",
        "from albumentations.pytorch import ToTensor\n",
        "from tqdm import tqdm_notebook, trange\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils import spectral_norm\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.utils import make_grid, save_image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5g4hYtDjORk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dL_uVVAm6-i0",
        "colab": {}
      },
      "source": [
        "if device == 'cuda':\n",
        "    !nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3SeOXShP3thi",
        "colab": {}
      },
      "source": [
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "seed_everything(config['seed'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GEA2rGvjORt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if config['use_google_drive']:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Zfpom1-TXvR9",
        "colab": {}
      },
      "source": [
        "BASE_PATH = '/content/drive/My Drive' if config['use_google_drive'] else str(pathlib.Path.home())\n",
        "DATASET_BASE_DIR = '/content' if config['use_google_drive'] else str(pathlib.Path.home())\n",
        "DATASET_ZIP = f'{BASE_PATH}/socofing.zip'\n",
        "DATASET_DIR = f'{DATASET_BASE_DIR}/socofing'\n",
        "DATASET_IMAGES = f'{DATASET_DIR}/SOCOFing/Real'\n",
        "\n",
        "if not os.path.exists(DATASET_DIR):\n",
        "    with zipfile.ZipFile(DATASET_ZIP, 'r') as file:\n",
        "        file.extractall(DATASET_DIR)\n",
        "\n",
        "OUT_BASE_PATH = f'{BASE_PATH}/BigGAN_outputs/{config[\"input_transformation\"]}'\n",
        "CHECKPOINTS_PATH = f'{OUT_BASE_PATH}/model'\n",
        "GEN_IMGS_PATH = f'{OUT_BASE_PATH}/images'\n",
        "\n",
        "os.makedirs(CHECKPOINTS_PATH, exist_ok=True)\n",
        "os.makedirs(GEN_IMGS_PATH, exist_ok=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFEGilWkjORy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_files = os.listdir(DATASET_IMAGES)\n",
        "all_files[:5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZefqzmBQ3thk"
      },
      "source": [
        "# Data processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "a3nWs7YgobFm",
        "colab": {}
      },
      "source": [
        "# functions for images visualization\n",
        "def examine_imgs(procces_img_func, data, columns=4, size=10, show_title=True):\n",
        "    rows = len(data) / columns + 1\n",
        "    figsize = (size*columns, size*rows)\n",
        "    f = plt.figure(figsize=figsize)\n",
        "    for i, image in enumerate(data):\n",
        "        img = procces_img_func(image)\n",
        "        sp = f.add_subplot(rows, columns, i+1)\n",
        "        sp.axis('Off')\n",
        "        if show_title:\n",
        "            title_text = str(i)\n",
        "            # if items in data are type of string\n",
        "            # if isinstance(image, str):\n",
        "            #     title_text += image\n",
        "            sp.set_title(title_text, fontsize=18)\n",
        "        plt.imshow(img)\n",
        "    plt.show()\n",
        "\n",
        "def identity(image):\n",
        "    return image\n",
        "\n",
        "def unnormalize(image):\n",
        "    # inverse operation for A.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    # WARNING!: works only for A.Normalize with exactly these used parameters\n",
        "    return np.round(np.clip(((image + 1)/2)*255,0,255)).astype(int)[...,::-1]\n",
        "\n",
        "def loader(image_path):\n",
        "    return cv2.imread(image_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sCSkLDKAOvAj"
      },
      "source": [
        "### Visualize some raw images\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ni15hVo0ML8A",
        "colab": {}
      },
      "source": [
        "num_to_show = 18\n",
        "full_paths = [os.path.join(DATASET_IMAGES, image_name) for image_name in all_files[:num_to_show]]\n",
        "examine_imgs(loader, full_paths, columns=6, size=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hPShYB_EPJ1O"
      },
      "source": [
        "### Strange borders\n",
        "\n",
        "As we can see on the pictures above, for some reason they have strange borders, so lets remove them:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oiJgHYb6P0pJ",
        "colab": {}
      },
      "source": [
        "first_img = cv2.imread(full_paths[0])\n",
        "plt.imshow(first_img)\n",
        "print(f'image shape: {first_img.shape}')\n",
        "print(\"Top left corner:\")\n",
        "pprint.pprint(first_img[:8,:8,1])\n",
        "print(\"Botom right corner:\")\n",
        "pprint.pprint(first_img[95:103,88:96,1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "i7T09VsCUizD"
      },
      "source": [
        "To get nice images we remove 2 pixels from top and left side, remove 4 pixel from bottom and right side."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "34vBXbvkQqI5",
        "colab": {}
      },
      "source": [
        "croped_img = first_img[2:99,2:92]\n",
        "plt.imshow(croped_img)\n",
        "print(f'image shape: {croped_img.shape}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0guHXh283th8",
        "colab": {}
      },
      "source": [
        "def load_image(file, transformation):\n",
        "    img = cv2.imread(os.path.join(DATASET_IMAGES, file))\n",
        "\n",
        "    # Resize - 64x64\n",
        "    if transformation == 'resize-64':\n",
        "        img = img[2:99,2:92]\n",
        "        transform = A.Compose([A.Resize(64, 64, interpolation=cv2.INTER_AREA),\n",
        "                               A.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "    # Crop - 64x64\n",
        "    elif transformation == 'crop-64':\n",
        "        img = img[2:99,2:92]\n",
        "        img = img[33:97,16:80]\n",
        "        transform = A.Compose([A.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "    # Crop - 80x80\n",
        "    elif transformation == 'crop-80':\n",
        "        img = img[2:99,2:92]\n",
        "        img = img[17:97,5:85] \n",
        "        transform = A.Compose([A.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "    # Crop - 96x96\n",
        "    elif transformation == 'crop-96':\n",
        "        img = img[3:99,:96]  \n",
        "        img[:,:2,:] = 0 # set right border to 0 (to be the same as the right side)\n",
        "        transform = A.Compose([A.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "    # Random Crop during augmentation\n",
        "    elif transformation == 'random-crop-64':\n",
        "        img = img[2:99,2:92]\n",
        "        transform = A.Compose([A.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "    else:\n",
        "        raise Error('Unknown input transformation specified.')\n",
        "\n",
        "    return transform(image=img)['image']\n",
        "\n",
        "all_images = [load_image(f, config['input_transformation']) for f in all_files]\n",
        "all_images = np.array(all_images)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oJnuw3cvo3uS",
        "colab": {}
      },
      "source": [
        "# For Debugging\n",
        "# first_img = cv2.imread(full_paths[0])\n",
        "# croped_img = first_img[3:99,:96]\n",
        "# croped_img[:,:2,:] = 0\n",
        "# plt.imshow(croped_img)\n",
        "# print(f'image shape: {croped_img.shape}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jeJVchy9Iem5",
        "colab": {}
      },
      "source": [
        "def extract_label(file):\n",
        "    _, label = file.split('__', 1)\n",
        "    label, _ = label.split('.', 1)\n",
        "    return label\n",
        "\n",
        "all_labels = [extract_label(f) for f in all_files]\n",
        "all_labels = LabelEncoder().fit_transform(all_labels)\n",
        "all_labels[:5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5eHwg_WF3tiB",
        "colab": {}
      },
      "source": [
        "class FingerprintDataset(Dataset):\n",
        "    def __init__(self, images, labels, transformation):\n",
        "        super().__init__()\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        # self.transform = A.Compose([A.HorizontalFlip(p=0.5), ToTensor()])\n",
        "        # # Crop random 64x64 patches\n",
        "        if transformation == 'random-crop-64':\n",
        "            self.transform = A.Compose([A.RandomCrop(height=64, width=64), ToTensor()])\n",
        "        else:\n",
        "            self.transform = A.Compose([ToTensor()])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, label = self.images[idx], self.labels[idx]\n",
        "        img = self.transform(image=img)['image']\n",
        "        label = torch.as_tensor(label, dtype=torch.long)\n",
        "\n",
        "        return img, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "234GdYw73tiE",
        "colab": {}
      },
      "source": [
        "# adding uniform noise works well.\n",
        "\n",
        "def get_dataiterator(images, labels, transformation, dataloader_params, device='cpu'):\n",
        "    train_dataset = FingerprintDataset(images, labels, transformation)\n",
        "    train_dataloader = DataLoader(train_dataset, **dataloader_params)\n",
        "    batch_size = dataloader_params['batch_size']\n",
        "\n",
        "    while True:\n",
        "        for imgs, labels in train_dataloader:\n",
        "            if batch_size != imgs.size(0):\n",
        "                break\n",
        "            else:\n",
        "                imgs, labels = imgs.to(device), labels.to(device)\n",
        "                imgs += (1.0 / 128.0) * torch.rand_like(imgs)\n",
        "\n",
        "                yield imgs, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-k1COhhe3tiG",
        "colab": {}
      },
      "source": [
        "# batch size around 64 ~ 128 improves score.\n",
        "# ~ 64 are too small, 128 ~ are too large (for 9 hours training). \n",
        "\n",
        "train_dataiterator = get_dataiterator(all_images, all_labels, config['input_transformation'], config['DataLoader'], device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rlE_bYeIWY-n"
      },
      "source": [
        "### Visualize some processed images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "c89lKNWgWkPj",
        "colab": {}
      },
      "source": [
        "real_imgs, _ = train_dataiterator.__next__()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIv3VIWdjOSS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "real_imgs[0].permute(1,2,0).shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qG7Im-ApUBMd",
        "colab": {}
      },
      "source": [
        "# TODO clip data to valid range...\n",
        "examine_imgs(identity, real_imgs.permute(0,2,3,1).add_(1).div_(2).cpu().detach().numpy()[:24], columns=6, size=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m4CiCkom3tiJ"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aacb_lN23tiK",
        "colab": {}
      },
      "source": [
        "# Attention slightly works.\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, channels, reduction_attn=8, reduction_sc=2):\n",
        "        super().__init__()\n",
        "        self.channles_attn = channels // reduction_attn\n",
        "        self.channels_sc = channels // reduction_sc\n",
        "        \n",
        "        self.conv_query = spectral_norm(nn.Conv2d(channels, self.channles_attn, kernel_size=1, bias=False))\n",
        "        self.conv_key = spectral_norm(nn.Conv2d(channels, self.channles_attn, kernel_size=1, bias=False))\n",
        "        self.conv_value = spectral_norm(nn.Conv2d(channels, self.channels_sc, kernel_size=1, bias=False))\n",
        "        self.conv_attn = spectral_norm(nn.Conv2d(self.channels_sc, channels, kernel_size=1, bias=False))\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))\n",
        "        \n",
        "        nn.init.orthogonal_(self.conv_query.weight.data)\n",
        "        nn.init.orthogonal_(self.conv_key.weight.data)\n",
        "        nn.init.orthogonal_(self.conv_value.weight.data)\n",
        "        nn.init.orthogonal_(self.conv_attn.weight.data)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch, _, h, w = x.size()\n",
        "        \n",
        "        proj_query = self.conv_query(x).view(batch, self.channles_attn, -1)\n",
        "        proj_key = F.max_pool2d(self.conv_key(x), 2).view(batch, self.channles_attn, -1)\n",
        "        \n",
        "        attn = torch.bmm(proj_key.permute(0,2,1), proj_query)\n",
        "        attn = F.softmax(attn, dim=1)\n",
        "        \n",
        "        proj_value = F.max_pool2d(self.conv_value(x), 2).view(batch, self.channels_sc, -1)\n",
        "        attn = torch.bmm(proj_value, attn)\n",
        "        attn = attn.view(batch, self.channels_sc, h, w)\n",
        "        attn = self.conv_attn(attn)\n",
        "        \n",
        "        out = self.gamma * attn + x\n",
        "        \n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dYcNw3E53tiM"
      },
      "source": [
        "## Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1PLNeKm93tiN",
        "colab": {}
      },
      "source": [
        "# using label information works well.\n",
        "# As for generator, it is realized by conditional batch normalization.\n",
        "\n",
        "class CBN2d(nn.Module):\n",
        "    def __init__(self, num_features, num_conditions):\n",
        "        super().__init__()\n",
        "        self.bn = nn.BatchNorm2d(num_features, affine=False)\n",
        "        self.embed = spectral_norm(nn.Conv2d(num_conditions, num_features*2, kernel_size=1, bias=False))\n",
        "        \n",
        "        nn.init.orthogonal_(self.embed.weight.data)\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        out = self.bn(x)\n",
        "        embed = self.embed(y.unsqueeze(2).unsqueeze(3))\n",
        "        gamma, beta = embed.chunk(2, dim=1)\n",
        "        out = (1.0 + gamma) * out + beta \n",
        "\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kSWEBcvL3tiR",
        "colab": {}
      },
      "source": [
        "# residual block improves convergence speed and generated image's quality.\n",
        "# nearest upsampling is better than others.\n",
        "\n",
        "class GBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, num_conditions, upsample=False):\n",
        "        super().__init__()\n",
        "        self.upsample = upsample\n",
        "        self.learnable_sc = in_channels != out_channels or upsample\n",
        "        \n",
        "        self.conv1 = spectral_norm(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False))\n",
        "        self.conv2 = spectral_norm(nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False))\n",
        "        self.cbn1 = CBN2d(in_channels, num_conditions)\n",
        "        self.cbn2 = CBN2d(out_channels, num_conditions)\n",
        "        if self.learnable_sc:\n",
        "            self.conv_sc = spectral_norm(nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False))\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        nn.init.orthogonal_(self.conv1.weight.data)\n",
        "        nn.init.orthogonal_(self.conv2.weight.data)\n",
        "        if self.learnable_sc:\n",
        "            nn.init.orthogonal_(self.conv_sc.weight.data)\n",
        "    \n",
        "    def _upsample_conv(self, x, conv):\n",
        "        x = F.interpolate(x, scale_factor=2, mode='nearest')\n",
        "        x = conv(x)\n",
        "        \n",
        "        return x\n",
        "    \n",
        "    def _residual(self, x, y):\n",
        "        x = self.relu(self.cbn1(x, y))\n",
        "        x = self._upsample_conv(x, self.conv1) if self.upsample else self.conv1(x)\n",
        "        x = self.relu(self.cbn2(x, y))\n",
        "        x = self.conv2(x)\n",
        "        \n",
        "        return x\n",
        "    \n",
        "    def _shortcut(self, x):\n",
        "        if self.learnable_sc:\n",
        "            x = self._upsample_conv(x, self.conv_sc) if self.upsample else self.conv_sc(x)\n",
        "            \n",
        "        return x\n",
        "    \n",
        "    def forward(self, x, y):\n",
        "        return self._shortcut(x) + self._residual(x, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OPBELHdd3tiV",
        "colab": {}
      },
      "source": [
        "# shared embedding of class labels, and hierarchical latent noise, work well.\n",
        "# this architecture is the same as BigGAN except for channel size.\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, latent_dim, ch, num_classes, embed_dim, use_attn=False):\n",
        "        super().__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.ch = ch\n",
        "        self.num_classes = num_classes\n",
        "        self.embed_dim = embed_dim\n",
        "        self.use_attn = use_attn\n",
        "        self.num_chunk = 5\n",
        "        num_latents = self.__get_num_latents()\n",
        "        \n",
        "        self.embed = nn.Embedding(num_classes, embed_dim)\n",
        "        self.fc = spectral_norm(nn.Linear(num_latents[0], ch*8*4*4, bias=False))\n",
        "        self.block1 = GBlock(ch*8, ch*8, num_latents[1], upsample=True)\n",
        "        self.block2 = GBlock(ch*8, ch*4, num_latents[2], upsample=True)\n",
        "        self.block3 = GBlock(ch*4, ch*2, num_latents[3], upsample=True)\n",
        "        if use_attn:\n",
        "            self.attn = Attention(ch*2)\n",
        "        self.block4 = GBlock(ch*2, ch, num_latents[4], upsample=True)\n",
        "        self.bn = nn.BatchNorm2d(ch)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.conv_last = spectral_norm(nn.Conv2d(ch, 3, kernel_size=3, padding=1, bias=False))\n",
        "        self.tanh = nn.Tanh()\n",
        "        \n",
        "        nn.init.orthogonal_(self.embed.weight.data)\n",
        "        nn.init.orthogonal_(self.fc.weight.data)\n",
        "        nn.init.orthogonal_(self.conv_last.weight.data)\n",
        "        nn.init.constant_(self.bn.weight.data, 1.0)\n",
        "        nn.init.constant_(self.bn.bias.data, 0.0)\n",
        "    \n",
        "    def __get_num_latents(self):\n",
        "        xs = torch.empty(self.latent_dim).chunk(self.num_chunk)\n",
        "        num_latents = [x.size(0) for x in xs]\n",
        "        for i in range(1, self.num_chunk):\n",
        "            num_latents[i] += self.embed_dim\n",
        "        \n",
        "        return num_latents\n",
        "    \n",
        "    def forward(self, x, y):\n",
        "        xs = x.chunk(self.num_chunk, dim=1)\n",
        "        y = self.embed(y)\n",
        "        \n",
        "        h = self.fc(xs[0])\n",
        "        h = h.view(h.size(0), self.ch*8, 4, 4)\n",
        "        h = self.block1(h, torch.cat([y, xs[1]], dim=1))\n",
        "        h = self.block2(h, torch.cat([y, xs[2]], dim=1))\n",
        "        h = self.block3(h, torch.cat([y, xs[3]], dim=1))\n",
        "        if self.use_attn:\n",
        "            h = self.attn(h)\n",
        "        h = self.block4(h, torch.cat([y, xs[4]], dim=1))\n",
        "        h = self.relu(self.bn(h))\n",
        "        out = self.tanh(self.conv_last(h))\n",
        "        \n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aLmv05yY3tiY"
      },
      "source": [
        "## Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jLh0glXD3tiY",
        "colab": {}
      },
      "source": [
        "# residual block improves convergence speed and generated image's quality.\n",
        "\n",
        "class DBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, downsample=False, optimized=False):\n",
        "        super().__init__()\n",
        "        self.downsample = downsample\n",
        "        self.optimized = optimized\n",
        "        self.learnable_sc = in_channels != out_channels or downsample\n",
        "        \n",
        "        self.conv1 = spectral_norm(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False))\n",
        "        self.conv2 = spectral_norm(nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False))\n",
        "        if self.learnable_sc:\n",
        "            self.conv_sc = spectral_norm(nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False))\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "        nn.init.orthogonal_(self.conv1.weight.data)\n",
        "        nn.init.orthogonal_(self.conv2.weight.data)\n",
        "        if self.learnable_sc:\n",
        "            nn.init.orthogonal_(self.conv_sc.weight.data)\n",
        "\n",
        "    def _residual(self, x):\n",
        "        if not self.optimized:\n",
        "            x = self.relu(x)\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        if self.downsample:\n",
        "            x = F.avg_pool2d(x, 2)\n",
        "        \n",
        "        return x\n",
        "    \n",
        "    def _shortcut(self, x):\n",
        "        if self.learnable_sc:\n",
        "            if self.optimized:\n",
        "                x = self.conv_sc(F.avg_pool2d(x, 2)) if self.downsample else self.conv_sc(x)\n",
        "            else:\n",
        "                x = F.avg_pool2d(self.conv_sc(x), 2) if self.downsample else self.conv_sc(x)\n",
        "        \n",
        "        return x\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self._shortcut(x) + self._residual(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DNwBrXUF3tib",
        "colab": {}
      },
      "source": [
        "# this architecture is the altered version of BigGAN Discriminator.\n",
        "# <- using residual block, projection.\n",
        "\n",
        "# but those points are different from original.\n",
        "# - reduce channel size.\n",
        "# - reduce model depth (remove last residual block).\n",
        "# - add minibatch stddev.\n",
        "# - with auxiliary classifier (ACGAN).\n",
        "#   <- improve image's quality and stabilize training.\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, ch, num_classes, use_attn=False):\n",
        "        super().__init__()\n",
        "        self.ch = ch\n",
        "        self.num_classes = num_classes\n",
        "        self.use_attn = use_attn\n",
        "        \n",
        "        self.block1 = DBlock(3, ch, downsample=True, optimized=True)\n",
        "        if use_attn:\n",
        "            self.attn = Attention(ch)\n",
        "        self.block2 = DBlock(ch, ch*2, downsample=True)\n",
        "        self.block3 = DBlock(ch*2, ch*4, downsample=True)\n",
        "        self.block4 = DBlock(ch*4, ch*8, downsample=True)\n",
        "        self.block5 = DBlock(ch*8+1, ch*8, downsample=False)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc = spectral_norm(nn.Linear(ch*8, 1, bias=False))\n",
        "        self.embed = spectral_norm(nn.Embedding(num_classes, ch*8))\n",
        "        self.clf = spectral_norm(nn.Linear(ch*8, num_classes, bias=False))\n",
        "        \n",
        "        nn.init.orthogonal_(self.fc.weight.data)\n",
        "        nn.init.orthogonal_(self.embed.weight.data)\n",
        "        nn.init.orthogonal_(self.clf.weight.data)\n",
        "    \n",
        "    def minibatch_stddev(self, x, group_size=4, eps=1e-8):\n",
        "        shape = x.size()\n",
        "        y = x.view(group_size, -1, shape[1], shape[2], shape[3])\n",
        "        y -= torch.mean(y, dim=0, keepdim=True)\n",
        "        y = torch.mean(y.pow(2), dim=0)\n",
        "        y = torch.sqrt(y + eps)\n",
        "        y = torch.mean(y, dim=[1,2,3], keepdim=True)\n",
        "        y = y.repeat(group_size, 1, shape[2], shape[3])\n",
        "\n",
        "        return torch.cat([x, y], dim=1)\n",
        "    \n",
        "    def forward(self, x, y):\n",
        "        h = self.block1(x)\n",
        "        if self.use_attn:\n",
        "            h = self.attn(h)\n",
        "        h = self.block2(h)\n",
        "        h = self.block3(h)\n",
        "        h = self.block4(h)\n",
        "        h = self.block5(self.minibatch_stddev(h))\n",
        "        h = self.relu(h)\n",
        "        h = torch.sum(h, dim=(2,3))\n",
        "        \n",
        "        out = self.fc(h)\n",
        "        out += torch.sum(self.embed(y)*h, dim=1, keepdim=True)\n",
        "        \n",
        "        ac = self.clf(h)\n",
        "        ac = F.log_softmax(ac, dim=1)\n",
        "        \n",
        "        return out, ac"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CWzoPV643tie"
      },
      "source": [
        "# Train GANs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1LRrH9Oj3tif",
        "colab": {}
      },
      "source": [
        "netG = Generator(**config['Generator']).to(device, torch.float32)\n",
        "netD = Discriminator(**config['Discriminator']).to(device, torch.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5AF1951T3tij",
        "colab": {}
      },
      "source": [
        "optim_G = Adam(params=netG.parameters(), lr=config['lr_G'], betas=config['betas'])\n",
        "optim_D = Adam(params=netD.parameters(), lr=config['lr_D'], betas=config['betas'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_ODyBXdx3til",
        "colab": {}
      },
      "source": [
        "decay_iter = config['num_iterations'] - config['decay_start_iteration']\n",
        "if decay_iter > 0:\n",
        "    lr_lambda_G = lambda x: (max(0,1-x/decay_iter))\n",
        "    lr_lambda_D = lambda x: (max(0,1-x/(decay_iter*config['d_steps'])))\n",
        "    lr_sche_G = LambdaLR(optim_G, lr_lambda=lr_lambda_G)\n",
        "    lr_sche_D = LambdaLR(optim_D, lr_lambda=lr_lambda_D)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lGL0vv8c3tin",
        "colab": {}
      },
      "source": [
        "def calc_advloss_D(real, fake, margin=1.0):\n",
        "    loss_real = torch.mean((real - fake.mean() - margin) ** 2)\n",
        "    loss_fake = torch.mean((fake - real.mean() + margin) ** 2)\n",
        "    loss = (loss_real + loss_fake) / 2\n",
        "    \n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1dFrRG2m3tiq",
        "colab": {}
      },
      "source": [
        "def calc_advloss_G(real, fake, margin=1.0):\n",
        "    loss_real = torch.mean((real - fake.mean() + margin) ** 2)\n",
        "    loss_fake = torch.mean((fake - real.mean() - margin) ** 2)\n",
        "    loss = (loss_real + loss_fake) / 2\n",
        "    \n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nMt5qr0H3tit",
        "colab": {}
      },
      "source": [
        "# auxiliary classifier loss.\n",
        "# this loss weighted by gamma (0.1) is added to adversarial loss.\n",
        "# coefficient gamma is quite sensitive.\n",
        "\n",
        "criterion = nn.NLLLoss().to(device, torch.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JmQrw4Fl3tiv",
        "colab": {}
      },
      "source": [
        "def sample_latents(batch_size, latent_dim, num_classes):\n",
        "    latents = torch.randn((batch_size, latent_dim), dtype=torch.float32, device=device)\n",
        "    labels = torch.randint(0, num_classes, size=(batch_size,), dtype=torch.long, device=device)\n",
        "    \n",
        "    return latents, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gfbQEUxW3ti3",
        "colab": {}
      },
      "source": [
        "def truncated_normal(size, threshold=2.0, dtype=torch.float32, device='cpu'):\n",
        "    x = scipy.stats.truncnorm.rvs(-threshold, threshold, size=size)\n",
        "    x = torch.from_numpy(x).to(device, dtype)\n",
        "\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "n3DC3L7e3ti5",
        "colab": {}
      },
      "source": [
        "def generate_eval_samples(generator, batch_size, latent_dim, num_classes):\n",
        "    latents = truncated_normal((batch_size, latent_dim), dtype=torch.float32, device=device)\n",
        "    labels =  torch.randint(0, num_classes, size=(batch_size,), dtype=torch.long, device=device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        imgs = (generator(latents, labels) + 1) / 2\n",
        "    \n",
        "    return imgs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PQs2Sxxllevp",
        "colab": {}
      },
      "source": [
        "def gen_and_save_some_images(step, repeatch_each_class=2, nrow=8):\n",
        "    split_size = 50\n",
        "\n",
        "    latent_dim = config['sample_latents']['latent_dim']\n",
        "    num_classes = config['sample_latents']['num_classes']\n",
        "\n",
        "    all_labels = torch.arange(num_classes, dtype=torch.long, device=device)\n",
        "    all_labels = all_labels.repeat_interleave(repeatch_each_class)\n",
        "    labels_split = all_labels.split(split_size)\n",
        "\n",
        "    imgs_list = []\n",
        "    for labels in labels_split:\n",
        "        batch_size = labels.size(0)\n",
        "        latents = truncated_normal((batch_size, latent_dim), threshold=1.5, dtype=torch.float32, device=device)\n",
        "        with torch.no_grad():\n",
        "            # imgs = (netGE(latents, labels) + 1) / 2\n",
        "            imgs = (netG(latents, labels) + 1) / 2\n",
        "            imgs_list.append(imgs)\n",
        "\n",
        "    all_imgs = torch.cat(imgs_list, dim=0)\n",
        "    all_imgs = make_grid(all_imgs, nrow=nrow, normalize=False)\n",
        "    all_imgs = all_imgs.mul_(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).to('cpu', torch.uint8).numpy()\n",
        "\n",
        "    img.imsave(f'{GEN_IMGS_PATH}/step-{step:05}.png', all_imgs)\n",
        "    # plt.figure(figsize=(2*nrow, 2*(num_classes*repeatch_each_class)//nrow))\n",
        "    # plt.imshow(all_imgs);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7ckJPp5DqCeV",
        "colab": {}
      },
      "source": [
        "def save_checkpoint(step, netG, netD, optim_G, optim_D):\n",
        "    torch.save({'step': step,\n",
        "                'netG_state_dict': netG.state_dict(),\n",
        "                'netD_state_dict': netD.state_dict(),\n",
        "                'optim_G_state_dict': optim_G.state_dict(),\n",
        "                'optim_D_state_dict': optim_D.state_dict(),\n",
        "                }, f'{CHECKPOINTS_PATH}/params-{step:05}.pt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yx7fjJqLjOS6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_last_checkpoint(netG, netD, optim_G, optim_D):\n",
        "    all_checkpoints = glob.glob(f'{CHECKPOINTS_PATH}/params-*.pt')\n",
        "\n",
        "    if not all_checkpoints:\n",
        "        return 0\n",
        "\n",
        "    all_checkpoints.sort(reverse=True)\n",
        "    last_checkpoint = torch.load(all_checkpoints[0])\n",
        "\n",
        "    netG.load_state_dict(last_checkpoint['netG_state_dict'])\n",
        "    netD.load_state_dict(last_checkpoint['netD_state_dict'])\n",
        "    optim_G.load_state_dict(last_checkpoint['optim_G_state_dict'])\n",
        "    optim_D.load_state_dict(last_checkpoint['optim_D_state_dict'])\n",
        "\n",
        "    return last_checkpoint['step']\n",
        "\n",
        "prev_step = load_last_checkpoint(netG, netD, optim_G, optim_D)\n",
        "print(f'prev_step: {prev_step}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "I74KzRqh3tiy",
        "colab": {}
      },
      "source": [
        "for step in trange(prev_step, config['num_iterations'], total=config['num_iterations'], initial=prev_step):\n",
        "    # Discriminator\n",
        "    for i in range(config['d_steps']):\n",
        "        for param in netD.parameters():\n",
        "            param.requires_grad_(True)\n",
        "\n",
        "        optim_D.zero_grad()\n",
        "\n",
        "        real_imgs, real_labels = train_dataiterator.__next__()\n",
        "        batch_size = real_imgs.size(0)\n",
        "\n",
        "        latents, fake_labels = sample_latents(batch_size, **config['sample_latents'])\n",
        "        fake_imgs = netG(latents, fake_labels).detach()\n",
        "\n",
        "        preds_real, preds_real_labels = netD(real_imgs, real_labels)\n",
        "        preds_fake, _ = netD(fake_imgs, fake_labels)\n",
        "\n",
        "        loss_D = calc_advloss_D(preds_real, preds_fake, config['margin'])\n",
        "        loss_D += config['gamma'] * criterion(preds_real_labels, real_labels)\n",
        "        loss_D.backward()\n",
        "        optim_D.step()\n",
        "\n",
        "        if (decay_iter > 0) and (step > config['decay_start_iteration']):\n",
        "            lr_sche_D.step()\n",
        "\n",
        "    # Generator\n",
        "    for param in netD.parameters():\n",
        "        param.requires_grad_(False)\n",
        "\n",
        "    optim_G.zero_grad()\n",
        "\n",
        "    real_imgs, real_labels = train_dataiterator.__next__()\n",
        "    batch_size = real_imgs.size(0)\n",
        "\n",
        "    latents, fake_labels = sample_latents(batch_size, **config['sample_latents'])\n",
        "    fake_imgs = netG(latents, fake_labels)\n",
        "\n",
        "    preds_real, _ = netD(real_imgs, real_labels)\n",
        "    preds_fake, preds_fake_labels = netD(fake_imgs, fake_labels)\n",
        "\n",
        "    loss_G = calc_advloss_G(preds_real, preds_fake, config['margin'])\n",
        "    loss_G += config['gamma'] * criterion(preds_fake_labels, fake_labels)\n",
        "    loss_G.backward()\n",
        "    optim_G.step()\n",
        "\n",
        "    if (decay_iter > 0) and (step > config['decay_start_iteration']):\n",
        "        lr_sche_G.step()\n",
        "\n",
        "    if step % 256 == 0:\n",
        "        gen_and_save_some_images(step)\n",
        "\n",
        "    if step % 512 == 0:\n",
        "        save_checkpoint(step, netG, netD, optim_G, optim_D)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RY0Nimsrx8Nd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}