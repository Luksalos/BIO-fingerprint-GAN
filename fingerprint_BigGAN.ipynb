{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "dog-generating-GAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbEW1V9etTtY",
        "colab_type": "text"
      },
      "source": [
        "Based on [this kaggle kernel](https://www.kaggle.com/yukia18/sub-rals-ac-biggan-with-minibatchstddev)  \n",
        "[model description](https://www.kaggle.com/c/generative-dog-images/discussion/104211#latest-601531)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoDhE2esx5Ig",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDeG1Z44yLJG",
        "colab_type": "text"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/Luksalos/BIO-fingerprint-GAN/blob/master/fingerprint_BigGAN.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/Luksalos/BIO-fingerprint-GAN/blob/master/fingerprint_BigGAN.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "13dGjbBW3tha",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import random\n",
        "import glob\n",
        "import shutil\n",
        "import warnings\n",
        "\n",
        "import cv2\n",
        "import scipy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import xml.etree.ElementTree as ET\n",
        "import albumentations as A\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from PIL import Image\n",
        "from albumentations.pytorch import ToTensor\n",
        "from tqdm import tqdm_notebook, trange\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils import spectral_norm\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.utils import make_grid, save_image\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dL_uVVAm6-i0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2zZnnLS95tD",
        "colab_type": "text"
      },
      "source": [
        "**Kaggle versions:**               \n",
        "torch                              1.3.0                \n",
        "torchtext                          0.4.0               \n",
        "torchvision                        0.4.1a0+d94043a "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5f6DKip48Cxx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip list | grep torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "collapsed": true,
        "id": "V85Omft53the",
        "colab_type": "text"
      },
      "source": [
        "# Settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIvmL1b23thf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = {'DataLoader': {'batch_size': 128,\n",
        "                         'shuffle': True},\n",
        "          'Generator': {'latent_dim': 120,\n",
        "                        'embed_dim': 32,\n",
        "                        'ch': 64,\n",
        "                        'num_classes': 120,\n",
        "                        'use_attn': True},\n",
        "          'Discriminator': {'ch': 64,\n",
        "                            'num_classes': 120,\n",
        "                            'use_attn': True},\n",
        "          'sample_latents': {'latent_dim': 120,\n",
        "                             'num_classes': 120},\n",
        "        #   'num_iterations': 26000,\n",
        "          'num_iterations': 5000,\n",
        "          'decay_start_iteration': 25000,\n",
        "          'd_steps': 1,\n",
        "          'lr_G': 2e-4,\n",
        "          'lr_D': 4e-4,\n",
        "          'betas': (0.0, 0.999),\n",
        "          'margin': 1.0,\n",
        "          'gamma': 0.1,\n",
        "          'ema': 0.999,\n",
        "          'seed': 42}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SeOXShP3thi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "seed_everything(config['seed'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USu-E8Abt8sc",
        "colab_type": "text"
      },
      "source": [
        "# Get data\n",
        "TODO\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xea_p3VKeQY2",
        "colab_type": "text"
      },
      "source": [
        "### Mount Google drive and unzip files \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zfpom1-TXvR9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJNY3pWyeBqs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "!unzip \"drive/My Drive/dogs.zip\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZefqzmBQ3thk",
        "colab_type": "text"
      },
      "source": [
        "# Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efo0XkKR3thm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "root_images = 'all-dogs/all-dogs/'\n",
        "root_annots = 'annotation/Annotation/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kGHuwQx3thp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_files = os.listdir(root_images)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UB1t8ILP3thr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "breeds = glob.glob(root_annots+'*')\n",
        "annotations = []\n",
        "for breed in breeds:\n",
        "    annotations += glob.glob(breed+'/*')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0li1PM6Q3tht",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "breed_map = {}\n",
        "for annotation in annotations:\n",
        "    breed = annotation.split('/')[-2]\n",
        "    index = breed.split('-')[0]\n",
        "    breed_map.setdefault(index, breed)\n",
        "# breed_map"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIkwJ8XY3thw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_labels = [breed_map[file.split('_')[0]] for file in all_files]\n",
        "le = LabelEncoder()\n",
        "all_labels = le.fit_transform(all_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmLePV8AgFcP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_labels[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCDvmoXqgy3_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_files[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbAUwfzG3thy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_bbox(file):\n",
        "    file = str(breed_map[file.split('_')[0]]) + '/' + str(file.split('.')[0])\n",
        "    path = os.path.join(root_annots, file)\n",
        "    tree = ET.parse(path)\n",
        "    root = tree.getroot()\n",
        "    objects = root.findall('object')\n",
        "    for o in objects:\n",
        "        bndbox = o.find('bndbox')\n",
        "        xmin = int(bndbox.find('xmin').text)\n",
        "        ymin = int(bndbox.find('ymin').text)\n",
        "        xmax = int(bndbox.find('xmax').text)\n",
        "        ymax = int(bndbox.find('ymax').text)\n",
        "    \n",
        "    return (xmin, ymin, xmax, ymax)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3d2BxB9t3th1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_bboxes = [load_bbox(file) for file in all_files]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6W4bWFs8lt0R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_bboxes[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJ8Q-cf73th3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make square bounding boxes of original ones,\n",
        "# to keep a dog's aspect ratio.\n",
        "\n",
        "def get_resized_bbox(height, width, bbox):\n",
        "    xmin, ymin, xmax, ymax = bbox\n",
        "    xlen = xmax - xmin\n",
        "    ylen = ymax - ymin\n",
        "\n",
        "    # print(f'{xmin}, {ymin}, {xmax}, {ymax}')\n",
        "    # print(f'xlen = {xlen}, ylen = {ylen}')\n",
        "    if xlen > ylen:\n",
        "        # print(\"xlen > ylen\")\n",
        "        diff = xlen - ylen\n",
        "        min_pad = min(ymin, diff//2)\n",
        "        max_pad = min(height-ymax, diff-min_pad)\n",
        "        ymin = ymin - min_pad\n",
        "        ymax = ymax + max_pad\n",
        "\n",
        "    elif ylen > xlen:\n",
        "        # print(\"ylen > xlen\")\n",
        "        diff = ylen - xlen\n",
        "        min_pad = min(xmin, diff//2)\n",
        "        max_pad = min(width-xmax, diff-min_pad)\n",
        "        xmin = xmin - min_pad\n",
        "        xmax = xmax + max_pad\n",
        "    # print(f'diff = {diff}, min_pad = {min_pad}, max_pad = {max_pad}, ymin = {ymin}, xmax = {xmax}')\n",
        "    # print(f'{xmin}, {ymin}, {xmax}, {ymax}')\n",
        "    \n",
        "    return xmin, ymin, xmax, ymax"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhTq10XFttNA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# file = all_files[0]\n",
        "# bbox = all_bboxes[0]\n",
        "# img = Image.open(os.path.join(root_images, file))\n",
        "# width, height = img.size\n",
        "# print(f'width = {width}, height = {height}')\n",
        "# xmin, ymin, xmax, ymax = get_resized_bbox(height, width, bbox)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79erGODT3th5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "resized_bboxes = []\n",
        "for file, bbox in zip(all_files, all_bboxes):\n",
        "    img = Image.open(os.path.join(root_images, file))\n",
        "    width, height = img.size\n",
        "    xmin, ymin, xmax, ymax = get_resized_bbox(height, width, bbox)\n",
        "    resized_bboxes.append((xmin, ymin, xmax, ymax))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZl7rx08l32s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "resized_bboxes[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0guHXh283th8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# crop by square bounding box -> resize and normalize.\n",
        "# cv2.INTER_AREA is better than others.\n",
        "\n",
        "def load_bboxcrop_resized_image(file, bbox):\n",
        "    img = cv2.imread(os.path.join(root_images, file))\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    xmin, ymin, xmax, ymax = bbox\n",
        "    img = img[ymin:ymax,xmin:xmax]\n",
        "\n",
        "    transform = A.Compose([A.Resize(64, 64, interpolation=cv2.INTER_AREA),\n",
        "                           A.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "    img = transform(image=img)['image']\n",
        "    \n",
        "    return img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ca3-R8Yb3th-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_images = [load_bboxcrop_resized_image(f, b) for f, b in zip(all_files, resized_bboxes)]\n",
        "all_images = np.array(all_images)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eHwg_WF3tiB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DogDataset(Dataset):\n",
        "    def __init__(self, images, labels):\n",
        "        super().__init__()\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.transform = A.Compose([A.HorizontalFlip(p=0.5), ToTensor()])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, label = self.images[idx], self.labels[idx]\n",
        "        img = self.transform(image=img)['image']\n",
        "        label = torch.as_tensor(label, dtype=torch.long)\n",
        "\n",
        "        return img, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "234GdYw73tiE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# adding uniform noise works well.\n",
        "\n",
        "def get_dataiterator(images, labels, dataloader_params, device='cpu'):\n",
        "    train_dataset = DogDataset(images, labels)\n",
        "    train_dataloader = DataLoader(train_dataset, **dataloader_params)\n",
        "    batch_size = dataloader_params['batch_size']\n",
        "\n",
        "    while True:\n",
        "        for imgs, labels in train_dataloader:\n",
        "            if batch_size != imgs.size(0):\n",
        "                break\n",
        "            else:\n",
        "                imgs, labels = imgs.to(device), labels.to(device)\n",
        "                imgs += (1.0 / 128.0) * torch.rand_like(imgs)\n",
        "\n",
        "                yield imgs, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-k1COhhe3tiG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# batch size around 64 ~ 128 improves score.\n",
        "# ~ 64 are too small, 128 ~ are too large (for 9 hours training). \n",
        "\n",
        "train_dataiterator = get_dataiterator(all_images, all_labels, config['DataLoader'], device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4CiCkom3tiJ",
        "colab_type": "text"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aacb_lN23tiK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Attention slightly works.\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, channels, reduction_attn=8, reduction_sc=2):\n",
        "        super().__init__()\n",
        "        self.channles_attn = channels // reduction_attn\n",
        "        self.channels_sc = channels // reduction_sc\n",
        "        \n",
        "        self.conv_query = spectral_norm(nn.Conv2d(channels, self.channles_attn, kernel_size=1, bias=False))\n",
        "        self.conv_key = spectral_norm(nn.Conv2d(channels, self.channles_attn, kernel_size=1, bias=False))\n",
        "        self.conv_value = spectral_norm(nn.Conv2d(channels, self.channels_sc, kernel_size=1, bias=False))\n",
        "        self.conv_attn = spectral_norm(nn.Conv2d(self.channels_sc, channels, kernel_size=1, bias=False))\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))\n",
        "        \n",
        "        nn.init.orthogonal_(self.conv_query.weight.data)\n",
        "        nn.init.orthogonal_(self.conv_key.weight.data)\n",
        "        nn.init.orthogonal_(self.conv_value.weight.data)\n",
        "        nn.init.orthogonal_(self.conv_attn.weight.data)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch, _, h, w = x.size()\n",
        "        \n",
        "        proj_query = self.conv_query(x).view(batch, self.channles_attn, -1)\n",
        "        proj_key = F.max_pool2d(self.conv_key(x), 2).view(batch, self.channles_attn, -1)\n",
        "        \n",
        "        attn = torch.bmm(proj_key.permute(0,2,1), proj_query)\n",
        "        attn = F.softmax(attn, dim=1)\n",
        "        \n",
        "        proj_value = F.max_pool2d(self.conv_value(x), 2).view(batch, self.channels_sc, -1)\n",
        "        attn = torch.bmm(proj_value, attn)\n",
        "        attn = attn.view(batch, self.channels_sc, h, w)\n",
        "        attn = self.conv_attn(attn)\n",
        "        \n",
        "        out = self.gamma * attn + x\n",
        "        \n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYcNw3E53tiM",
        "colab_type": "text"
      },
      "source": [
        "## Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PLNeKm93tiN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# using label information works well.\n",
        "# As for generator, it is realized by conditional batch normalization.\n",
        "\n",
        "class CBN2d(nn.Module):\n",
        "    def __init__(self, num_features, num_conditions):\n",
        "        super().__init__()\n",
        "        self.bn = nn.BatchNorm2d(num_features, affine=False)\n",
        "        self.embed = spectral_norm(nn.Conv2d(num_conditions, num_features*2, kernel_size=1, bias=False))\n",
        "        \n",
        "        nn.init.orthogonal_(self.embed.weight.data)\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        out = self.bn(x)\n",
        "        embed = self.embed(y.unsqueeze(2).unsqueeze(3))\n",
        "        gamma, beta = embed.chunk(2, dim=1)\n",
        "        out = (1.0 + gamma) * out + beta \n",
        "\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSWEBcvL3tiR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# residual block improves convergence speed and generated image's quality.\n",
        "# nearest upsampling is better than others.\n",
        "\n",
        "class GBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, num_conditions, upsample=False):\n",
        "        super().__init__()\n",
        "        self.upsample = upsample\n",
        "        self.learnable_sc = in_channels != out_channels or upsample\n",
        "        \n",
        "        self.conv1 = spectral_norm(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False))\n",
        "        self.conv2 = spectral_norm(nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False))\n",
        "        self.cbn1 = CBN2d(in_channels, num_conditions)\n",
        "        self.cbn2 = CBN2d(out_channels, num_conditions)\n",
        "        if self.learnable_sc:\n",
        "            self.conv_sc = spectral_norm(nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False))\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        nn.init.orthogonal_(self.conv1.weight.data)\n",
        "        nn.init.orthogonal_(self.conv2.weight.data)\n",
        "        if self.learnable_sc:\n",
        "            nn.init.orthogonal_(self.conv_sc.weight.data)\n",
        "    \n",
        "    def _upsample_conv(self, x, conv):\n",
        "        x = F.interpolate(x, scale_factor=2, mode='nearest')\n",
        "        x = conv(x)\n",
        "        \n",
        "        return x\n",
        "    \n",
        "    def _residual(self, x, y):\n",
        "        x = self.relu(self.cbn1(x, y))\n",
        "        x = self._upsample_conv(x, self.conv1) if self.upsample else self.conv1(x)\n",
        "        x = self.relu(self.cbn2(x, y))\n",
        "        x = self.conv2(x)\n",
        "        \n",
        "        return x\n",
        "    \n",
        "    def _shortcut(self, x):\n",
        "        if self.learnable_sc:\n",
        "            x = self._upsample_conv(x, self.conv_sc) if self.upsample else self.conv_sc(x)\n",
        "            \n",
        "        return x\n",
        "    \n",
        "    def forward(self, x, y):\n",
        "        return self._shortcut(x) + self._residual(x, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPBELHdd3tiV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# shared embedding of class labels, and hierarchical latent noise, work well.\n",
        "# this architecture is the same as BigGAN except for channel size.\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, latent_dim, ch, num_classes, embed_dim, use_attn=False):\n",
        "        super().__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.ch = ch\n",
        "        self.num_classes = num_classes\n",
        "        self.embed_dim = embed_dim\n",
        "        self.use_attn = use_attn\n",
        "        self.num_chunk = 5\n",
        "        num_latents = self.__get_num_latents()\n",
        "        \n",
        "        self.embed = nn.Embedding(num_classes, embed_dim)\n",
        "        self.fc = spectral_norm(nn.Linear(num_latents[0], ch*8*4*4, bias=False))\n",
        "        self.block1 = GBlock(ch*8, ch*8, num_latents[1], upsample=True)\n",
        "        self.block2 = GBlock(ch*8, ch*4, num_latents[2], upsample=True)\n",
        "        self.block3 = GBlock(ch*4, ch*2, num_latents[3], upsample=True)\n",
        "        if use_attn:\n",
        "            self.attn = Attention(ch*2)\n",
        "        self.block4 = GBlock(ch*2, ch, num_latents[4], upsample=True)\n",
        "        self.bn = nn.BatchNorm2d(ch)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.conv_last = spectral_norm(nn.Conv2d(ch, 3, kernel_size=3, padding=1, bias=False))\n",
        "        self.tanh = nn.Tanh()\n",
        "        \n",
        "        nn.init.orthogonal_(self.embed.weight.data)\n",
        "        nn.init.orthogonal_(self.fc.weight.data)\n",
        "        nn.init.orthogonal_(self.conv_last.weight.data)\n",
        "        nn.init.constant_(self.bn.weight.data, 1.0)\n",
        "        nn.init.constant_(self.bn.bias.data, 0.0)\n",
        "    \n",
        "    def __get_num_latents(self):\n",
        "        xs = torch.empty(self.latent_dim).chunk(self.num_chunk)\n",
        "        num_latents = [x.size(0) for x in xs]\n",
        "        for i in range(1, self.num_chunk):\n",
        "            num_latents[i] += self.embed_dim\n",
        "        \n",
        "        return num_latents\n",
        "    \n",
        "    def forward(self, x, y):\n",
        "        xs = x.chunk(self.num_chunk, dim=1)\n",
        "        y = self.embed(y)\n",
        "        \n",
        "        h = self.fc(xs[0])\n",
        "        h = h.view(h.size(0), self.ch*8, 4, 4)\n",
        "        h = self.block1(h, torch.cat([y, xs[1]], dim=1))\n",
        "        h = self.block2(h, torch.cat([y, xs[2]], dim=1))\n",
        "        h = self.block3(h, torch.cat([y, xs[3]], dim=1))\n",
        "        if self.use_attn:\n",
        "            h = self.attn(h)\n",
        "        h = self.block4(h, torch.cat([y, xs[4]], dim=1))\n",
        "        h = self.relu(self.bn(h))\n",
        "        out = self.tanh(self.conv_last(h))\n",
        "        \n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLmv05yY3tiY",
        "colab_type": "text"
      },
      "source": [
        "## Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLh0glXD3tiY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# residual block improves convergence speed and generated image's quality.\n",
        "\n",
        "class DBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, downsample=False, optimized=False):\n",
        "        super().__init__()\n",
        "        self.downsample = downsample\n",
        "        self.optimized = optimized\n",
        "        self.learnable_sc = in_channels != out_channels or downsample\n",
        "        \n",
        "        self.conv1 = spectral_norm(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False))\n",
        "        self.conv2 = spectral_norm(nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False))\n",
        "        if self.learnable_sc:\n",
        "            self.conv_sc = spectral_norm(nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False))\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "        nn.init.orthogonal_(self.conv1.weight.data)\n",
        "        nn.init.orthogonal_(self.conv2.weight.data)\n",
        "        if self.learnable_sc:\n",
        "            nn.init.orthogonal_(self.conv_sc.weight.data)\n",
        "\n",
        "    def _residual(self, x):\n",
        "        if not self.optimized:\n",
        "            x = self.relu(x)\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        if self.downsample:\n",
        "            x = F.avg_pool2d(x, 2)\n",
        "        \n",
        "        return x\n",
        "    \n",
        "    def _shortcut(self, x):\n",
        "        if self.learnable_sc:\n",
        "            if self.optimized:\n",
        "                x = self.conv_sc(F.avg_pool2d(x, 2)) if self.downsample else self.conv_sc(x)\n",
        "            else:\n",
        "                x = F.avg_pool2d(self.conv_sc(x), 2) if self.downsample else self.conv_sc(x)\n",
        "        \n",
        "        return x\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self._shortcut(x) + self._residual(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNwBrXUF3tib",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this architecture is the altered version of BigGAN Discriminator.\n",
        "# <- using residual block, projection.\n",
        "\n",
        "# but those points are different from original.\n",
        "# - reduce channel size.\n",
        "# - reduce model depth (remove last residual block).\n",
        "# - add minibatch stddev.\n",
        "# - with auxiliary classifier (ACGAN).\n",
        "#   <- improve image's quality and stabilize training.\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, ch, num_classes, use_attn=False):\n",
        "        super().__init__()\n",
        "        self.ch = ch\n",
        "        self.num_classes = num_classes\n",
        "        self.use_attn = use_attn\n",
        "        \n",
        "        self.block1 = DBlock(3, ch, downsample=True, optimized=True)\n",
        "        if use_attn:\n",
        "            self.attn = Attention(ch)\n",
        "        self.block2 = DBlock(ch, ch*2, downsample=True)\n",
        "        self.block3 = DBlock(ch*2, ch*4, downsample=True)\n",
        "        self.block4 = DBlock(ch*4, ch*8, downsample=True)\n",
        "        self.block5 = DBlock(ch*8+1, ch*8, downsample=False)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc = spectral_norm(nn.Linear(ch*8, 1, bias=False))\n",
        "        self.embed = spectral_norm(nn.Embedding(num_classes, ch*8))\n",
        "        self.clf = spectral_norm(nn.Linear(ch*8, num_classes, bias=False))\n",
        "        \n",
        "        nn.init.orthogonal_(self.fc.weight.data)\n",
        "        nn.init.orthogonal_(self.embed.weight.data)\n",
        "        nn.init.orthogonal_(self.clf.weight.data)\n",
        "    \n",
        "    def minibatch_stddev(self, x, group_size=4, eps=1e-8):\n",
        "        shape = x.size()\n",
        "        y = x.view(group_size, -1, shape[1], shape[2], shape[3])\n",
        "        y -= torch.mean(y, dim=0, keepdim=True)\n",
        "        y = torch.mean(y.pow(2), dim=0)\n",
        "        y = torch.sqrt(y + eps)\n",
        "        y = torch.mean(y, dim=[1,2,3], keepdim=True)\n",
        "        y = y.repeat(group_size, 1, shape[2], shape[3])\n",
        "\n",
        "        return torch.cat([x, y], dim=1)\n",
        "    \n",
        "    def forward(self, x, y):\n",
        "        h = self.block1(x)\n",
        "        if self.use_attn:\n",
        "            h = self.attn(h)\n",
        "        h = self.block2(h)\n",
        "        h = self.block3(h)\n",
        "        h = self.block4(h)\n",
        "        h = self.block5(self.minibatch_stddev(h))\n",
        "        h = self.relu(h)\n",
        "        h = torch.sum(h, dim=(2,3))\n",
        "        \n",
        "        out = self.fc(h)\n",
        "        out += torch.sum(self.embed(y)*h, dim=1, keepdim=True)\n",
        "        \n",
        "        ac = self.clf(h)\n",
        "        ac = F.log_softmax(ac, dim=1)\n",
        "        \n",
        "        return out, ac"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWzoPV643tie",
        "colab_type": "text"
      },
      "source": [
        "# Train GANs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LRrH9Oj3tif",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "netG = Generator(**config['Generator']).to(device, torch.float32)\n",
        "netD = Discriminator(**config['Discriminator']).to(device, torch.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmJ7HzhJ3tih",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Exponential moving average of generator weights works well.\n",
        "# Got error, when updating netGE buffers\n",
        "\n",
        "netGE = Generator(**config['Generator']).to(device, torch.float32)\n",
        "netGE.load_state_dict(netG.state_dict());"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AF1951T3tij",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optim_G = Adam(params=netG.parameters(), lr=config['lr_G'], betas=config['betas'])\n",
        "optim_D = Adam(params=netD.parameters(), lr=config['lr_D'], betas=config['betas'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ODyBXdx3til",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decay_iter = config['num_iterations'] - config['decay_start_iteration']\n",
        "if decay_iter > 0:\n",
        "    lr_lambda_G = lambda x: (max(0,1-x/decay_iter))\n",
        "    lr_lambda_D = lambda x: (max(0,1-x/(decay_iter*config['d_steps'])))\n",
        "    lr_sche_G = LambdaLR(optim_G, lr_lambda=lr_lambda_G)\n",
        "    lr_sche_D = LambdaLR(optim_D, lr_lambda=lr_lambda_D)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGL0vv8c3tin",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calc_advloss_D(real, fake, margin=1.0):\n",
        "    loss_real = torch.mean((real - fake.mean() - margin) ** 2)\n",
        "    loss_fake = torch.mean((fake - real.mean() + margin) ** 2)\n",
        "    loss = (loss_real + loss_fake) / 2\n",
        "    \n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dFrRG2m3tiq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calc_advloss_G(real, fake, margin=1.0):\n",
        "    loss_real = torch.mean((real - fake.mean() + margin) ** 2)\n",
        "    loss_fake = torch.mean((fake - real.mean() - margin) ** 2)\n",
        "    loss = (loss_real + loss_fake) / 2\n",
        "    \n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMt5qr0H3tit",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# auxiliary classifier loss.\n",
        "# this loss weighted by gamma (0.1) is added to adversarial loss.\n",
        "# coefficient gamma is quite sensitive.\n",
        "\n",
        "criterion = nn.NLLLoss().to(device, torch.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmQrw4Fl3tiv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample_latents(batch_size, latent_dim, num_classes):\n",
        "    latents = torch.randn((batch_size, latent_dim), dtype=torch.float32, device=device)\n",
        "    labels = torch.randint(0, num_classes, size=(batch_size,), dtype=torch.long, device=device)\n",
        "    \n",
        "    return latents, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cyiqh4EXN14h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Only for testing\n",
        "# counter = 0\n",
        "# for buffer_G, buffer_GE in zip(netG.buffers(), netGE.buffers()):\n",
        "#     if counter == 8:\n",
        "#         print(buffer_G)\n",
        "#         print(buffer_GE.data.mul_(1).add_(0*buffer_G.data))\n",
        "#         print(buffer_G)\n",
        "#     print(counter)\n",
        "#     # buffer_GE.data.mul_(config['ema']).add_((1-config['ema'])*buffer_G.data)\n",
        "    \n",
        "#     counter += 1\n",
        "#     # buffer_GE.data.mul_(config['ema']).add_((1-config['ema'])*buffer_G.data)\n",
        "# counter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I74KzRqh3tiy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for step in trange(1, config['num_iterations']):\n",
        "    # Discriminator\n",
        "    for i in range(config['d_steps']):\n",
        "        for param in netD.parameters():\n",
        "            param.requires_grad_(True)\n",
        "    \n",
        "        optim_D.zero_grad()\n",
        "\n",
        "        real_imgs, real_labels = train_dataiterator.__next__()\n",
        "        batch_size = real_imgs.size(0)\n",
        "\n",
        "        latents, fake_labels = sample_latents(batch_size, **config['sample_latents'])\n",
        "        fake_imgs = netG(latents, fake_labels).detach()\n",
        "        \n",
        "        preds_real, preds_real_labels = netD(real_imgs, real_labels)\n",
        "        preds_fake, _ = netD(fake_imgs, fake_labels)\n",
        "\n",
        "        loss_D = calc_advloss_D(preds_real, preds_fake, config['margin'])\n",
        "        loss_D += config['gamma'] * criterion(preds_real_labels, real_labels)\n",
        "        loss_D.backward()\n",
        "        optim_D.step()\n",
        "        \n",
        "        if (decay_iter > 0) and (step > config['decay_start_iteration']):\n",
        "            lr_sche_D.step()\n",
        "\n",
        "    # Generator\n",
        "    for param in netD.parameters():\n",
        "        param.requires_grad_(False)\n",
        "\n",
        "    optim_G.zero_grad()\n",
        "    \n",
        "    real_imgs, real_labels = train_dataiterator.__next__()\n",
        "    batch_size = real_imgs.size(0)\n",
        "    \n",
        "    latents, fake_labels = sample_latents(batch_size, **config['sample_latents'])\n",
        "    fake_imgs = netG(latents, fake_labels)\n",
        "\n",
        "    preds_real, _ = netD(real_imgs, real_labels)\n",
        "    preds_fake, preds_fake_labels = netD(fake_imgs, fake_labels)\n",
        "\n",
        "    loss_G = calc_advloss_G(preds_real, preds_fake, config['margin'])\n",
        "    loss_G += config['gamma'] * criterion(preds_fake_labels, fake_labels)\n",
        "    loss_G.backward()\n",
        "    optim_G.step()\n",
        "    \n",
        "    if (decay_iter > 0) and (step > config['decay_start_iteration']):\n",
        "        lr_sche_G.step()\n",
        "    \n",
        "    # Got this error: result type Float can't be cast to the desired output type Long\n",
        "    # Exponential Moving Average (EMA) - https://openreview.net/forum?id=SJgw_sRqFQ\n",
        "    # Update Generator Eval\n",
        "    # for param_G, param_GE in zip(netG.parameters(), netGE.parameters()):\n",
        "    #     param_GE.data.mul_(config['ema']).add_((1-config['ema'])*param_G.data)\n",
        "    # for buffer_G, buffer_GE in zip(netG.buffers(), netGE.buffers()):\n",
        "    #     buffer_GE.data.mul_(config['ema']).add_((1-config['ema'])*buffer_G.data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXTgF6cf3ti2",
        "colab_type": "text"
      },
      "source": [
        "# Generate Samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfbQEUxW3ti3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def truncated_normal(size, threshold=2.0, dtype=torch.float32, device='cpu'):\n",
        "    x = scipy.stats.truncnorm.rvs(-threshold, threshold, size=size)\n",
        "    x = torch.from_numpy(x).to(device, dtype)\n",
        "\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3DC3L7e3ti5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_eval_samples(generator, batch_size, latent_dim, num_classes):\n",
        "    latents = truncated_normal((batch_size, latent_dim), dtype=torch.float32, device=device)\n",
        "    labels =  torch.randint(0, num_classes, size=(batch_size,), dtype=torch.long, device=device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        imgs = (generator(latents, labels) + 1) / 2\n",
        "    \n",
        "    return imgs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7vmanVJ3ti7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_submissions(generator, user_images_unzipped_path, latent_dim, num_classes):\n",
        "    if not os.path.exists(user_images_unzipped_path):\n",
        "        os.mkdir(user_images_unzipped_path)\n",
        "    \n",
        "    sample_batch_size = 50\n",
        "    num_samples = 10000\n",
        "    \n",
        "    for i in range(0, num_samples, sample_batch_size):\n",
        "        imgs = generate_eval_samples(generator, sample_batch_size, latent_dim, num_classes)\n",
        "        for j, img in enumerate(imgs):\n",
        "            save_image(img, os.path.join(user_images_unzipped_path, f'image_{i+j:05d}.png'))\n",
        "    \n",
        "    shutil.make_archive('images', 'zip', user_images_unzipped_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCt0eFAO3ti-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# user_images_unzipped_path = '../output_images'\n",
        "# make_submissions(netGE, user_images_unzipped_path, **config['sample_latents'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8YOyXYk3tjA",
        "colab_type": "text"
      },
      "source": [
        "# See some examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqevW5sq3tjA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "repeat_breeds = 5\n",
        "nrow = 15\n",
        "split_size = 50\n",
        "\n",
        "latent_dim = config['sample_latents']['latent_dim']\n",
        "num_classes = config['sample_latents']['num_classes']\n",
        "\n",
        "all_labels = torch.arange(num_classes, dtype=torch.long, device=device)\n",
        "all_labels = all_labels.repeat_interleave(repeat_breeds)\n",
        "labels_split = all_labels.split(split_size)\n",
        "\n",
        "imgs_list = []\n",
        "for labels in labels_split:\n",
        "    batch_size = labels.size(0)\n",
        "    latents = truncated_normal((batch_size, latent_dim), threshold=1.5, dtype=torch.float32, device=device)\n",
        "    with torch.no_grad():\n",
        "        # imgs = (netGE(latents, labels) + 1) / 2\n",
        "        imgs = (netG(latents, labels) + 1) / 2\n",
        "        imgs_list.append(imgs)\n",
        "    \n",
        "all_imgs = torch.cat(imgs_list, dim=0)\n",
        "all_imgs = make_grid(all_imgs, nrow=nrow, normalize=False)\n",
        "all_imgs = all_imgs.mul_(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).to('cpu', torch.uint8).numpy()\n",
        "\n",
        "plt.figure(figsize=(2*nrow, 2*(num_classes*repeat_breeds)//nrow))\n",
        "plt.imshow(all_imgs);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPA1_M8P3tjG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}